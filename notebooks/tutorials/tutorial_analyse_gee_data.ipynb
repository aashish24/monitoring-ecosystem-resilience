{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Analyse data downloaded from Google Earth Engine using pyveg \n",
    "\n",
    "\n",
    "Google Earth Engine is a powerful tool for obtaining and analysing satellite imagery. The pyveg package provides useful scripts for interacting with the Earth Engine API and downloading data. \n",
    "\n",
    "The location used in this tutorial is Tiger Bush vegetation from Niger in coordinates 2.59, 13.12. The downloaded data is a JSON file containing weather and network centrality metrics in a monthly basis from 2015 to 2020.\n",
    "\n",
    "Now lets use the functions provided by pyveg to run a simple analysis on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from pyveg.src.data_analysis_utils import *\n",
    "from pyveg.src.plotting import *\n",
    "from pyveg.src.image_utils import create_gif_from_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input dataset is a json file found in this directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results directory from `download_gee_data` script.\n",
    "json_summary_path =  'results_summary_TigerBush_Niger.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output figures will be saved in an `analysis` sub-directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put output plots in the results dir\n",
    "input_dir = '.'\n",
    "output_dir = os.path.join(input_dir, 'analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read all json files in the directory and produce a dictionary of dataframes. Each key is a satellite, either weather related or image related (for the network centrality measures).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading results from '/Users/crangelsmith/PycharmProjects/monitoring-ecosystem-resilience/notebooks/tutorials/results_summary_TigerBush_Niger.json'...\n",
      "dict_keys(['COPERNICUS/S2', 'ECMWF/ERA5/MONTHLY'])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Reading results from '{os.path.abspath(json_summary_path)}'...\")\n",
    "dfs = variable_read_json_to_dataframe(json_summary_path)\n",
    "\n",
    "print (dfs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the output dataframe look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date                                        feature_vec  latitude  \\\n",
      "0  2015-07-15  [0.0, -31.0, -59.0, -71.0, -116.0, -130.0, -22...    2.6377   \n",
      "1  2015-07-15  [0.0, -17.0, -69.0, -104.0, -169.0, -200.0, -2...    2.5514   \n",
      "2  2015-07-15  [0.0, -49.0, -103.0, -171.0, -249.0, -292.0, -...    2.5741   \n",
      "3  2015-07-15  [0.0, -13.0, -39.0, -62.0, -103.0, -132.0, -20...    2.5923   \n",
      "4  2015-07-15  [0.0, -33.0, -81.0, -112.0, -164.0, -226.0, -2...    2.6286   \n",
      "\n",
      "   longitude    mean  offset  offset50  slope         std  \n",
      "0    13.1086 -456.70 -1082.0    -640.0 -54.10  343.613897  \n",
      "1    13.1223 -459.10 -1026.0    -568.0 -51.30  318.552492  \n",
      "2    13.1405 -621.65 -1325.0    -684.0 -66.25  406.338440  \n",
      "3    13.1359 -398.95  -919.0    -527.0 -45.95  292.988818  \n",
      "4    13.1495 -516.35 -1145.0    -617.0 -57.25  357.583176  \n",
      "         date  mean_2m_air_temperature  total_precipitation\n",
      "0  2015-01-16               296.405121             0.000000\n",
      "1  2015-02-15               303.000031             0.002291\n",
      "2  2015-04-16               305.954498             0.000000\n",
      "3  2015-05-16               307.894501             0.003323\n",
      "4  2015-06-15               306.375061             0.034950\n"
     ]
    }
   ],
   "source": [
    "print (dfs['COPERNICUS/S2'].head())\n",
    "print (dfs['ECMWF/ERA5/MONTHLY'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial analysis\n",
    "\n",
    "First, lets build 2D plots showing the network centrality values on the general 10km images for each date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating spatial plots...\n"
     ]
    }
   ],
   "source": [
    "# spatial analysis and plotting \n",
    "# from the dataframe, produce network metric figure for each avalaible date\n",
    "print('\\nCreating spatial plots...')\n",
    "\n",
    "# create new subdir for time series analysis\n",
    "spatial_subdir = os.path.join(output_dir, 'spatial')\n",
    "if not os.path.exists(spatial_subdir):\n",
    "    os.makedirs(spatial_subdir, exist_ok=True)\n",
    "\n",
    "for collection_name, df in dfs.items():\n",
    "    if collection_name == 'COPERNICUS/S2' or 'LANDSAT' in collection_name:\n",
    "        # convert the dataframe of each image to geopandas and coarse its resolution slightly\n",
    "        data_df_geo = convert_to_geopandas(df.copy())\n",
    "        data_df_geo_coarse = coarse_dataframe(data_df_geo, 4)\n",
    "        create_lat_long_metric_figures(data_df_geo_coarse, 'offset50', spatial_subdir)\n",
    "\n",
    "output_plots_name = create_gif_from_images(spatial_subdir,'output.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualise the result on a GIF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "with open(name,'rb') as f:\n",
    "    display(Image(data=f.read(), format='png',width=500, height=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average network centrality feature vectors ver all time points and sub images are the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new subdir for time series analysis\n",
    "tsa_subdir = output_dir\n",
    "\n",
    " # remove outliers from the time series\n",
    "dfs = drop_veg_outliers(dfs, sigmas=3) # not convinced this is really helping much\n",
    "\n",
    "# plot the feature vectors averaged over all time points and sub images\n",
    "try:\n",
    "    plot_feature_vectors(dfs, tsa_subdir)\n",
    "except AttributeError:\n",
    "    print('Can not plot feature vectors...') \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series analysis\n",
    "\n",
    "Using the data we can build a time series. For this analysis we do the following steps:\n",
    "\n",
    "- Build time series for every sub-image, we drop points with large outliers and smooth the sub-image time series.\n",
    "- We average all the network centrality measures from every sub-image into a single time series.\n",
    "- Compare time series with precipitation data and calculate measures such a correlation, AR1, Kendal tau, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dataframe to time series\n",
    "time_series_dfs = make_time_series(dfs.copy())\n",
    "\n",
    "# LOESS smoothing on sub-image time series\n",
    "smoothed_time_series_dfs = make_time_series(smooth_veg_data(dfs.copy(), n=4)) # increase smoothing with n>5\n",
    "\n",
    "# make a smoothed time series plot\n",
    "plot_smoothed_time_series(smoothed_time_series_dfs, tsa_subdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore auto-correlation between both smoothed and un-smoothed time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make autocorrelation plots\n",
    "plot_autocorrelation_function(smoothed_time_series_dfs, tsa_subdir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate the cross-correlation between the network centrality measures and precipitation for different lags of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make cross correlation scatterplot matrix plots\n",
    "plot_cross_correlations(smoothed_time_series_dfs, tsa_subdir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seasonal and trend analysis\n",
    "\n",
    "The time series shown above show a clear seasonal trend. The STL decomposition implementation from the statsmodels package is applied to the un-smoothed time series to separate the different components. \n",
    "\n",
    "This is done for both the network centrality metrics and precipitation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_stl_decomposition(time_series_dfs,12,tsa_subdir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time series is detrended by a first order differentiation (substracting the values from the last month in the 12 period). \n",
    "\n",
    "The same time series analysis (AR1, correlation, Kendal Tau)is then done on the de-trended time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   remove seasonality in the summary time series\n",
    "time_series_uns_summary_dfs = remove_seasonality_combined(smoothed_time_series_dfs.copy(), 12, \"M\")\n",
    "\n",
    "# make a smoothed time series plot\n",
    "plot_smoothed_time_series(time_series_uns_summary_dfs, tsa_subdir, '-no-seasonality-summary-ts',plot_std = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
